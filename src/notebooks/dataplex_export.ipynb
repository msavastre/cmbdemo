{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dataplex Metadata Export Demo (Self-Contained)\n",
                "\n",
                "This notebook demonstrates how to export Dataplex metadata and create a BigQuery reporting table. \n",
                "All logic is contained within this notebook for ease of use.\n",
                "\n",
                "**Configuration is defined via variables in the cell below.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Any\n",
                "\n",
                "from google.cloud import dataplex_v1\n",
                "from google.cloud import storage\n",
                "from google.cloud import bigquery"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration Variables\n",
                "\n",
                "Set your project details here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "PROJECT_ID = \"your-project-id\"\n",
                "LOCATION = \"us-central1\"\n",
                "\n",
                "# Dataplex Export Scope\n",
                "DATAPLEX_PROJECT_IDS = [\"your-project-id\"]\n",
                "DATAPLEX_ITEM_TYPES = [\"ASPECT_TYPE\", \"ENTRY\"] # Options: ASPECT_TYPE, ENTRY, ASPECT, FULL\n",
                "\n",
                "# GCS Output\n",
                "GCS_BUCKET_NAME = \"your-bucket-name\"\n",
                "GCS_OUTPUT_PATH = \"dataplex-export/\"\n",
                "\n",
                "# BigQuery Reporting Table\n",
                "BQ_DATASET_ID = \"your_dataset_id\"\n",
                "# ---------------------"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Config Classes\n",
                "\n",
                "Helper classes to structure the configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class DataplexConfig:\n",
                "    scope: Dict[str, Any]\n",
                "\n",
                "@dataclass\n",
                "class GCSConfig:\n",
                "    bucket_name: str\n",
                "    output_path: str\n",
                "\n",
                "@dataclass\n",
                "class BigQueryConfig:\n",
                "    dataset_id: str\n",
                "\n",
                "# Instantiate Config Objects from Variables\n",
                "dataplex_config = DataplexConfig(scope={\n",
                "    \"project_ids\": DATAPLEX_PROJECT_IDS,\n",
                "    \"item_types\": DATAPLEX_ITEM_TYPES\n",
                "})\n",
                "\n",
                "gcs_config = GCSConfig(\n",
                "    bucket_name=GCS_BUCKET_NAME,\n",
                "    output_path=GCS_OUTPUT_PATH\n",
                ")\n",
                "\n",
                "bq_config = BigQueryConfig(\n",
                "    dataset_id=BQ_DATASET_ID\n",
                ")\n",
                "\n",
                "print(f\"Configuration loaded for project: {PROJECT_ID}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataplex Logic\n",
                "\n",
                "Functions to trigger the metadata export job."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def trigger_export_job(project_id: str, location: str, dataplex_config: DataplexConfig, gcs_config: GCSConfig) -> str:\n",
                "    \"\"\"Triggers a Dataplex metadata export job.\"\"\"\n",
                "    client = dataplex_v1.CatalogServiceClient()\n",
                "    \n",
                "    parent = f\"projects/{project_id}/locations/{location}\"\n",
                "    \n",
                "    metadata_job = dataplex_v1.MetadataJob()\n",
                "    metadata_job.type_ = dataplex_v1.MetadataJob.Type.EXPORT\n",
                "    \n",
                "    export_spec = dataplex_v1.MetadataJob.ExportSpec()\n",
                "    export_spec.output_path = f\"gs://{gcs_config.bucket_name}/{gcs_config.output_path}\"\n",
                "    \n",
                "    # Construct the Scope object manually to be safe and support item_types\n",
                "    scope_msg = dataplex_v1.MetadataJob.ExportSpec.Scope()\n",
                "    \n",
                "    if 'project_ids' in dataplex_config.scope:\n",
                "        scope_msg.project_ids.extend(dataplex_config.scope['project_ids'])\n",
                "        \n",
                "    if 'item_types' in dataplex_config.scope:\n",
                "        # Mapping strings to enums\n",
                "        item_type_map = {\n",
                "            \"ASPECT_TYPE\": dataplex_v1.MetadataJob.ExportSpec.Scope.ItemType.ASPECT_TYPE,\n",
                "            \"ENTRY\": dataplex_v1.MetadataJob.ExportSpec.Scope.ItemType.ENTRY,\n",
                "            \"ASPECT\": dataplex_v1.MetadataJob.ExportSpec.Scope.ItemType.ASPECT,\n",
                "            \"FULL\": dataplex_v1.MetadataJob.ExportSpec.Scope.ItemType.FULL\n",
                "        }\n",
                "        \n",
                "        for it in dataplex_config.scope['item_types']:\n",
                "            if it in item_type_map:\n",
                "                scope_msg.item_types.append(item_type_map[it])\n",
                "            else:\n",
                "                print(f\"Warning: Unknown item type {it}, skipping.\")\n",
                "\n",
                "    export_spec.scope = scope_msg\n",
                "    \n",
                "    request = dataplex_v1.CreateMetadataJobRequest(\n",
                "        parent=parent,\n",
                "        metadata_job=metadata_job\n",
                "    )\n",
                "    \n",
                "    metadata_job.export_spec = export_spec\n",
                "    \n",
                "    operation = client.create_metadata_job(request=request)\n",
                "    print(\"Triggered metadata export job...\")\n",
                "    response = operation.result() # Wait for completion\n",
                "    \n",
                "    return response.name"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GCS Logic\n",
                "\n",
                "Functions to read exported metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def read_exported_metadata(project_id: str, config: GCSConfig) -> List[Dict[str, Any]]:\n",
                "    \"\"\"Reads exported metadata JSONL files from GCS.\"\"\"\n",
                "    client = storage.Client(project=project_id)\n",
                "    bucket = client.bucket(config.bucket_name)\n",
                "    \n",
                "    blobs = bucket.list_blobs(prefix=config.output_path)\n",
                "    \n",
                "    metadata = []\n",
                "    for blob in blobs:\n",
                "        if blob.name.endswith('.jsonl'):\n",
                "            print(f\"Reading {blob.name}...\")\n",
                "            content = blob.download_as_text()\n",
                "            for line in content.splitlines():\n",
                "                if line.strip():\n",
                "                    metadata.append(json.loads(line))\n",
                "                    \n",
                "    return metadata"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## BigQuery Logic\n",
                "\n",
                "Functions to create the BigLake reporting table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_metadata_reporting_table(project_id: str, location: str, bq_config: BigQueryConfig, gcs_config: GCSConfig):\n",
                "    \"\"\"Creates a BigLake table for reporting on exported Dataplex metadata.\"\"\"\n",
                "    client = bigquery.Client(project=project_id, location=location)\n",
                "    \n",
                "    dataset_ref = bigquery.DatasetReference(project_id, bq_config.dataset_id)\n",
                "    \n",
                "    # Ensure dataset exists\n",
                "    try:\n",
                "        client.get_dataset(dataset_ref)\n",
                "    except Exception:\n",
                "        dataset = bigquery.Dataset(dataset_ref)\n",
                "        dataset.location = location\n",
                "        client.create_dataset(dataset)\n",
                "        print(f\"Created dataset {bq_config.dataset_id}\")\n",
                "\n",
                "    table_name = \"dataplex_metadata_export\"\n",
                "    table_ref = dataset_ref.table(table_name)\n",
                "    \n",
                "    # Construct ExternalConfig for BigLake\n",
                "    source_uri = f\"gs://{gcs_config.bucket_name}/{gcs_config.output_path.rstrip('/')}/*\"\n",
                "    \n",
                "    external_config = bigquery.ExternalConfig(\"HIVE_PARTITIONING\")\n",
                "    external_config.source_uris = [source_uri]\n",
                "    external_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
                "    \n",
                "    # Configure Hive partitioning\n",
                "    external_config.hive_partitioning = bigquery.HivePartitioningOptions()\n",
                "    external_config.hive_partitioning.mode = \"AUTO\"\n",
                "    external_config.hive_partitioning.source_uri_prefix = f\"gs://{gcs_config.bucket_name}/{gcs_config.output_path.rstrip('/')}\"\n",
                "\n",
                "    table = bigquery.Table(table_ref)\n",
                "    table.external_data_configuration = external_config\n",
                "    \n",
                "    # Try to create (or update)\n",
                "    try:\n",
                "        client.create_table(table, exists_ok=True)\n",
                "        print(f\"Created/Updated BigLake table {bq_config.dataset_id}.{table_name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to create table {table_name}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execution\n",
                "\n",
                "Trigger job, verify data, and create table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trigger Job\n",
                "print(\"Triggering Dataplex metadata export job...\")\n",
                "# job_name = trigger_export_job(PROJECT_ID, LOCATION, dataplex_config, gcs_config)\n",
                "# print(f\"Job triggered: {job_name}\")\n",
                "\n",
                "# Verify Data\n",
                "print(\"Reading exported metadata from GCS...\")\n",
                "metadata = read_exported_metadata(PROJECT_ID, gcs_config)\n",
                "print(f\"Read {len(metadata)} metadata items.\")\n",
                "if metadata:\n",
                "    print(\"Sample item:\")\n",
                "    print(json.dumps(metadata[0], indent=2))\n",
                "\n",
                "# Create Table\n",
                "print(\"Creating BigLake table for metadata reporting...\")\n",
                "create_metadata_reporting_table(PROJECT_ID, LOCATION, bq_config, gcs_config)\n",
                "print(\"Done.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}